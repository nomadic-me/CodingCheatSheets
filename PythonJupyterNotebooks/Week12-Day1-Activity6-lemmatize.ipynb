{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Students Do: Lemmatize\n",
    "In this activity, create a function that performs stopwording, regex cleaning of non-letter characters, word tokenizing, and lemmatization on each word in the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import reuters, stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUNGARY RAISES PRICES IN EFFORT TO CURB DEFICIT\n",
      "  Hungary has announced sharp price\n",
      "  increases for a range of food and consumer products as part of\n",
      "  its efforts to curb a soaring budget deficit.\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "article = reuters.raw(fileids=reuters.fileids(categories='cpi')[2])\n",
    "print(article[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function\n",
    "def process_text_without_lemmatizing(article):\n",
    "    # Define a set of stopwords using `stopwords.words()`\n",
    "    sw = set(stopwords.words('english'))\n",
    "    # Create custom stopwords\n",
    "    sw_addons = {'said', 'sent', 'found', 'including', 'today', 'announced', 'week', 'basically', 'also'}\n",
    "    # Define the regex parameters\n",
    "    regex = re.compile(\"[^a-zA-Z ]\")\n",
    "    # Apply regex parameters to article\n",
    "    re_clean = regex.sub('', article)\n",
    "    # Apply `word_tokenize` to the regex scrubbed text\n",
    "    words = word_tokenize(re_clean)\n",
    "    # Create list of lower-case words that are not in the stopword set\n",
    "    output = [word.lower() for word in words if word.lower() not in sw.union(sw_addons)]\n",
    "    # Convert the tokenized words to lower case while lemmatizing\n",
    "    result = [lemmatizer.lemmatize(word) for word in output]\n",
    "    # Return the final list\n",
    "    return output\n",
    "\n",
    "def process_text_with_lemmatizing(article):\n",
    "    # Define a set of stopwords using `stopwords.words()`\n",
    "    sw = set(stopwords.words('english'))\n",
    "    # Create custom stopwords\n",
    "    sw_addons = {'said', 'sent', 'found', 'including', 'today', 'announced', 'week', 'basically', 'also'}\n",
    "    # Define the regex parameters\n",
    "    regex = re.compile(\"[^a-zA-Z ]\")\n",
    "    # Apply regex parameters to article\n",
    "    re_clean = regex.sub('', article)\n",
    "    # Apply `word_tokenize` to the regex scrubbed text\n",
    "    words = word_tokenize(re_clean)\n",
    "    # Create list of lower-case words that are not in the stopword set\n",
    "    result = [lemmatizer.lemmatize(word) for word in words]\n",
    "    output = [word.lower() for word in result if word.lower() not in sw.union(sw_addons)]\n",
    "    # Convert the tokenized words to lower case while lemmatizing\n",
    "    # Return the final list\n",
    "    return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE Lemmatizing:\n",
      "\n",
      "['hungary', 'raises', 'prices', 'effort', 'curb', 'deficit', 'hungary', 'sharp', 'price', 'increases', 'range', 'food', 'consumer', 'products', 'part', 'efforts', 'curb', 'soaring', 'budget', 'deficit', 'official', 'mti', 'news', 'agency', 'government', 'decided', 'consumer', 'price', 'subsidies', 'cut', 'reduce', 'state', 'spending', 'price', 'meat', 'rise', 'average', 'pct', 'beer', 'spirits', 'pct', 'mti', 'mti', 'consumer', 'goods', 'become', 'expensive', 'price', 'refrigerators', 'rising', 'five', 'pct', 'number', 'measures', 'ease', 'hardship', 'higher', 'pensions', 'family', 'allowances', 'statistics', 'indicate', 'budget', 'deficit', 'tripled', 'billion', 'forints', 'central', 'banker', 'janos', 'fekete', 'finance', 'ministry', 'trying', 'cut', 'shortfall', 'billion', 'planned', 'billion', 'major', 'tax', 'reform', 'introduction', 'westernstyle', 'valued', 'added', 'tax', 'planned', 'january', 'effort', 'cure', 'problems', 'state', 'spending', 'diplomats', 'latest', 'announcement', 'shows', 'authorities', 'forced', 'act', 'quickly', 'keep', 'years', 'deficit', 'control', 'measures', 'aimed', 'cooling', 'overheated', 'economy', 'could', 'help', 'dampen', 'hungarians', 'appetite', 'imported', 'western', 'goods', 'consume', 'increasingly', 'expensive', 'hard', 'currency', 'diplomats', 'diplomats', 'however', 'expect', 'kind', 'social', 'unrest', 'followed', 'sharp', 'price', 'rises', 'east', 'bloc', 'states', 'notably', 'poland'] \n",
      "\n",
      "AFTER Lemmatizing:\n",
      "\n",
      "['hungary', 'raises', 'prices', 'effort', 'curb', 'deficit', 'hungary', 'ha', 'sharp', 'price', 'increase', 'range', 'food', 'consumer', 'product', 'part', 'effort', 'curb', 'soaring', 'budget', 'deficit', 'official', 'mti', 'news', 'agency', 'government', 'decided', 'consumer', 'price', 'subsidy', 'cut', 'reduce', 'state', 'spending', 'price', 'meat', 'rise', 'average', 'pct', 'beer', 'spirit', 'pct', 'mti', 'mti', 'consumer', 'good', 'become', 'expensive', 'price', 'refrigerator', 'rising', 'five', 'pct', 'number', 'measure', 'ease', 'hardship', 'higher', 'pension', 'family', 'allowance', 'statistics', 'indicate', 'budget', 'deficit', 'tripled', 'billion', 'forint', 'central', 'banker', 'janos', 'fekete', 'ha', 'finance', 'ministry', 'trying', 'cut', 'shortfall', 'billion', 'planned', 'billion', 'major', 'tax', 'reform', 'introduction', 'westernstyle', 'valued', 'added', 'tax', 'planned', 'january', 'effort', 'cure', 'problem', 'state', 'spending', 'diplomat', 'latest', 'announcement', 'show', 'authority', 'forced', 'act', 'quickly', 'keep', 'year', 'deficit', 'control', 'measure', 'aimed', 'cooling', 'overheated', 'economy', 'could', 'help', 'dampen', 'hungarians', 'appetite', 'imported', 'western', 'good', 'consume', 'increasingly', 'expensive', 'hard', 'currency', 'diplomat', 'diplomat', 'however', 'expect', 'kind', 'social', 'unrest', 'followed', 'sharp', 'price', 'rise', 'east', 'bloc', 'state', 'notably', 'poland']\n"
     ]
    }
   ],
   "source": [
    "#Print Results\n",
    "print(f'BEFORE Lemmatizing:\\n\\n{process_text_without_lemmatizing(article)} \\n\\nAFTER Lemmatizing:\\n\\n{process_text_with_lemmatizing(article)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE Lemmatizing:\n",
      "\n",
      "140 \n",
      "\n",
      "AFTER Lemmatizing:\n",
      "\n",
      "142\n"
     ]
    }
   ],
   "source": [
    "#Print Results\n",
    "print(f'BEFORE Lemmatizing:\\n\\n{len(process_text_without_lemmatizing(article))} \\n\\nAFTER Lemmatizing:\\n\\n{len(process_text_with_lemmatizing(article))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
